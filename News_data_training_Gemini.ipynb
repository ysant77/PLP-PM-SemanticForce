{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UbqmNNTqNVR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/news_analysis_summary_combined_dedup.xlsx')"
      ],
      "metadata": {
        "id": "dquwYHaXN31p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "DuP6qaTOQxAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "SLQpaD-UH6ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "4pNxiKD1IMtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "ACvtWXvpQ8Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxrjiv1qMQ_U"
      },
      "outputs": [],
      "source": [
        "def summarize_with_api(text, model):\n",
        "  \"\"\"\n",
        "  Summarizes the text using the provided model (placeholder).\n",
        "\n",
        "  **Note:** This is a basic example and might require adjustments based on the specific model implementation.\n",
        "\n",
        "  Args:\n",
        "      text (str): The text to summarize.\n",
        "      model (object): The model object used for summarization.\n",
        "\n",
        "  Returns:\n",
        "      str: The summarized text or None if an error occurs.\n",
        "  \"\"\"\n",
        "  output_text = \"\"\n",
        "  chunks = split_into_chunks(text)\n",
        "  print(len(chunks))\n",
        "  idx = 0\n",
        "  for chunk in chunks:\n",
        "    retries = 0\n",
        "    idx += 1\n",
        "    while retries < 3:  # Set a maximum number of retries\n",
        "      try:\n",
        "        print(f\"chunk {idx} starting\")\n",
        "        response = model.generate_content(\"Please summarize the text: \" + chunk)\n",
        "        output_text += response.text\n",
        "        output_text += \" \"\n",
        "        print(f\"chunk {idx} completed\")\n",
        "        time.sleep(5)\n",
        "        retries += 3\n",
        "      except Exception as ex:\n",
        "        print(f\"Error occurred during summarization: {ex}\")\n",
        "        retries += 1\n",
        "        time.sleep(2**retries)  # Exponential backoff for retries\n",
        "\n",
        "    #print(f\"Failed to summarize text after {retries} retries.\")\n",
        "    #return None\n",
        "  return output_text.strip()\n",
        "\n",
        "def split_into_chunks(text, max_tokens=12288):\n",
        "    # Function to split the text into chunks based on max_tokens\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for word in words:\n",
        "        if current_length + len(word) + 1 > max_tokens:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "            current_length = len(word)\n",
        "        else:\n",
        "            current_chunk.append(word)\n",
        "            current_length += len(word) + 1  # +1 for the space\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_indices = list(range(len(df)))\n",
        "keys = list(df.keys())\n",
        "#completed_indices = [int(ele) for ele in keys]\n",
        "#remaining_indices = list(set(all_indices) - set(completed_indices) )\n",
        "remaining_indices = list(set(all_indices) - set(keys))"
      ],
      "metadata": {
        "id": "DFs8MGK2QDtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = {}"
      ],
      "metadata": {
        "id": "INU2Ff5XUye6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 200\n",
        "start_index = 0\n",
        "\n",
        "for end_index in range(start_index + batch_size, len(remaining_indices) + 1, batch_size):\n",
        "  batch_indices = remaining_indices[start_index:end_index]\n",
        "  batch_df = pd.DataFrame(columns=['article_text', 'Summary'])  # Create empty dataframe\n",
        "\n",
        "  for idx in batch_indices:\n",
        "    text = df.iloc[idx]['article_text']\n",
        "    time.sleep(5)  # This might not be necessary depending on the API rate limits\n",
        "    res = summarize_with_api(text, model)\n",
        "    if res:\n",
        "      # Assuming 'article_text' is available and summary can be added as a new column\n",
        "      batch_df.loc[len(batch_df)] = [text, res]  # Append data as a new row\n",
        "      print(f'{idx} completed')\n",
        "    else:\n",
        "      print(f'{idx} failed')\n",
        "      continue\n",
        "\n",
        "  # Update main output dictionary and save batch output as dataframe\n",
        "  output.update({str(idx): res for idx, res in batch_df['Summary'].items()})  #\n",
        "  batch_filename = f\"batch_{start_index}_{end_index}.xlsx\"\n",
        "  batch_df.to_excel(batch_filename, index=False)  # Save dataframe to file\n",
        "\n",
        "  start_index = end_index\n",
        "df.to_excel('/content/drive/MyDrive/Colab_Notebooks/news_summary.xlsx', index=False)"
      ],
      "metadata": {
        "id": "K1gYMyNoM3Zt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}