{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750ce1b-0076-4330-ab62-9b2d4f555745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install GoogleNews\n",
    "import GoogleNews\n",
    "from GoogleNews import GoogleNews\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import requests\n",
    "!pip install newspaper3k\n",
    "import newspaper # library to extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c17fd2-0671-4c5b-93eb-18f6ae250f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_text(url):\n",
    "    try:\n",
    "        article = newspaper.Article(url=url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text\n",
    "    except:\n",
    "        return 'URL cannot be opened or read'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d35955-bb42-42c7-9e18-cec424e41d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline, AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForTokenClassification, AutoModelForSeq2SeqLM\n",
    "\n",
    "def get_org(doc):\n",
    "\n",
    "  # Specify bert-base-ner model for both tokenizer and model\n",
    "  ner_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "  ner_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "  nlp = pipeline('ner', model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "  res=[]\n",
    "  if not doc:\n",
    "    return\n",
    "  d=nlp(doc)\n",
    "  for entity in d:\n",
    "    if entity['entity_group']=='ORG':\n",
    "      org=entity['word'].strip().lstrip()\n",
    "      if org not in res and org!='.':\n",
    "        res.append(org)\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e87696-fc90-4464-9523-027f828db975",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def get_sentiment_label(sentiment_scores):\n",
    "    \n",
    "    compound_score = sentiment_scores['compound']\n",
    "    if compound_score >= 0.52:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.48:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "def add_sentiment_analysis(df, text_column='article_text', sentiment_column='sentiment'):\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sentiment_scores = df[text_column].apply(analyzer.polarity_scores)  # Apply analyzer directly\n",
    "    df[sentiment_column] = sentiment_scores\n",
    "    df['sentiment_label'] = df[sentiment_column].apply(get_sentiment_label)  # Assign labels\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdbb4b-71cc-4568-95a7-e69439c31b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "gn = GoogleNews(lang='en', region='US', encode='utf-8')\n",
    "\n",
    "# input query & duration here\n",
    "\n",
    "query = 'Tesla Inc' # company name\n",
    "start_date='01/07/2024'\n",
    "end_date='01/08/2024'\n",
    "\n",
    "output_file_name=f\"{query}_{start_date}_{end_date}.xlsx\"\n",
    "\n",
    "datetime_list=pd.date_range(start_date,end_date,freq='d').to_list()\n",
    "date_list=[d.strftime('%m/%d/%Y') for d in datetime_list]\n",
    "n_date=len(date_list)\n",
    "\n",
    "fail_count=0\n",
    "\n",
    "for i in range(n_date-1):\n",
    "    \n",
    "    gn.set_time_range(start_date,end_date)\n",
    "    gn.get_news(query)\n",
    "    # Limit the number of retrieved news articles to 50\n",
    "    results = gn.results(sort=True)[:50] \n",
    "\n",
    "# Add \"http://\" to each link\n",
    "df=pd.DataFrame(results)\n",
    "df['link'] = 'http://' + df['link'] \n",
    "\n",
    "# Add a column for extracted text\n",
    "df['article_text'] = df['link'].apply(extract_article_text)\n",
    "\n",
    "# Add a column to apply the NER function\n",
    "df['extracted organizations'] = df['article_text'].apply(lambda text: get_org(text))\n",
    "\n",
    "# Add sentiment analysis with sentiment labels\n",
    "df = add_sentiment_analysis(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8904fc22-4f6b-47eb-8ba5-ad317603482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to remove duplicate or similar news\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "def get_vector(sentence):\n",
    "    if not sentence:\n",
    "        sentences=['']\n",
    "    else:\n",
    "        sentences=[sentence]\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings[0]\n",
    "\n",
    "def get_similarity(df):\n",
    "    # calculate the max similarity with earlier news\n",
    "    length=len(df)\n",
    "    unique=[]\n",
    "    news2datetime=dict(zip(df.index,df.datetime))\n",
    "    if length<=1:\n",
    "        return\n",
    "    else:\n",
    "        for i in range(1,length):\n",
    "            max_sim=0\n",
    "            for j in range (0,i):\n",
    "                if abs(news2datetime[i]-news2datetime[j]).days<14:\n",
    "                    curr_sim=cosine_similarity(df.loc[i]['vector'].reshape(1,-1),df.loc[j]['vector'].reshape(1,-1))[0][0]\n",
    "                    max_sim=max(max_sim,curr_sim)\n",
    "                    # below code for manual check\n",
    "#                     if curr_sim>0.9:\n",
    "#                         print(i, j)\n",
    "#                         print(df.loc[i]['article_text'])\n",
    "#                         print('====')\n",
    "#                         print(df.loc[j]['article_text'])\n",
    "            if max_sim<0.9:\n",
    "\n",
    "                unique.append(i)\n",
    "    df2=df.loc[unique]\n",
    "    df2.reset_index(inplace=True)\n",
    "    return df2\n",
    "\n",
    "df.dropna(subset='article_text',inplace=True)\n",
    "df = df[df['article_text'] != 'URL cannot be opened or read']\n",
    "df.sort_values(by='date', ascending=True, inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df['vector']=df['article_text'].apply(lambda x: get_vector(x))\n",
    "# deduplicate\n",
    "df2=get_similarity(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
