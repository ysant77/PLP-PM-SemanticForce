{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [03:16<00:00, 98.06s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.31s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "\n",
    "modelName = \"yatharth97/yatharth-gemma-2b-it-isa-v2\"\n",
    "\n",
    "bnbConfig = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelName,\n",
    "    device_map = {\"\": 0},\n",
    "    quantization_config=bnbConfig\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(modelName)#, padding_side='left')\n",
    "tokenizer.padding_side='left'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# device = 0 if torch.cuda.is_available() else -1  # GPU if available, otherwise CPU\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"yatharth97/llama-2-7b-chat-yatharth-v2\",device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(system_prompt, user_query):\n",
    "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
    "\n",
    "    :param data_point: dict: Data point\n",
    "    :return: dict: tokenzed prompt\n",
    "    \"\"\"\n",
    "    text = f\"\"\"\n",
    "    <|system|>{system_prompt}<|user|> {user_query}<|assistant|>\n",
    "    \"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "As a highly intelligent assistant and successor of google gemma model, your primary goal is to provide accurate, relevant, and context-aware responses to\n",
    "user queries based on the provided information. Ensure your answers are factual, free from bias, and avoid promoting violence, hate speech, or any form\n",
    "of discrimination. Focus on assisting the user effectively and safely\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  <start_of_turn>user\n",
    "  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "  {query}\n",
    "  <end_of_turn>\\n<start_of_turn>model\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, early_stopping=True, max_new_tokens=512, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "  # decoded = tokenizer.batch_decode(generated_ids)\n",
    "  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "  return (decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gemma_env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n  user\\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n  Give me stock market details for Google over the last 5m.\\n  \\nmodel\\n\\n\\n  \\n    user As a highly intelligent assistant and successor of google gemma model, your primary goal is to provide accurate, relevant, and context-aware responses to user queries based on the provided information. Ensure your answers are factual, free from bias, and avoid promoting violence, hate speech, or any form of discrimination. Focus on assisting the user effectively and safely.  Give me stock market details for Google over the last 5m. \\n\\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m \\n    \\n    model stock_info:Google:5m'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"Give me stock market details for Google over the last 5m.\"\n",
    "\n",
    "get_completion(question, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_prompt(system_prompt, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.to(device)\n",
    "custom_pipeline = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    <|system|>\n",
      "As a highly intelligent assistant and successor of google gemma model, your primary goal is to provide accurate, relevant, and context-aware responses to\n",
      "user queries based on the provided information. Ensure your answers are factual, free from bias, and avoid promoting violence, hate speech, or any form\n",
      "of discrimination. Focus on assisting the user effectively and safely\n",
      "<|user|> Give me stock market details for Google over the last 5m.<|assistant|>\n",
      "    <|assistant|> Give me stock market details for Google over the last 5m.<|user|>\n",
      "    <|user|> Google stock market details are ready for assistance.<|assistant|>\n",
      "    <|assistant|> Google stock market details are ready for assistance.<|user|>\n",
      "    <|assistant|> Google stock market details are helpful for user queries and are ready for assistance.<|user|>\n",
      "    <|assistant|> Google stock market details are helpful for user queries and are ready for assistance.<|assistant|>\n",
      "    <|assistant|> Google stock market details are helpful for user queries and are ready for assistance.<|assistant|>\n",
      "    <|assistant|> Google stock market details are helpful for user queries and are ready for assistance.<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = custom_pipeline(prompt, max_length=256)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
