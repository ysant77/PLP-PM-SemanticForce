{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mdjyKqBcVIAM","outputId":"733b7161-c6fa-4fcc-f98f-0c05dadeec6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers[torch]\n","  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n","Collecting filelock (from transformers[torch])\n","  Using cached filelock-3.13.4-py3-none-any.whl.metadata (2.8 kB)\n","Collecting huggingface-hub<1.0,>=0.19.3 (from transformers[torch])\n","  Using cached huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n","Collecting numpy>=1.17 (from transformers[torch])\n","  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/t5_env/lib/python3.10/site-packages (from transformers[torch]) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/t5_env/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n","Collecting regex!=2019.12.17 (from transformers[torch])\n","  Downloading regex-2024.4.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m925.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /opt/conda/envs/t5_env/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n","Collecting tokenizers<0.19,>=0.14 (from transformers[torch])\n","  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting safetensors>=0.4.1 (from transformers[torch])\n","  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/t5_env/lib/python3.10/site-packages (from transformers[torch]) (4.66.2)\n","Collecting torch (from transformers[torch])\n","  Using cached torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","Collecting accelerate>=0.21.0 (from transformers[torch])\n","  Using cached accelerate-0.29.2-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: psutil in /opt/conda/envs/t5_env/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n","Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers[torch])\n","  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/t5_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n","Collecting sympy (from torch->transformers[torch])\n","  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n","Collecting networkx (from torch->transformers[torch])\n","  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n","Requirement already satisfied: jinja2 in /opt/conda/envs/t5_env/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.3)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch->transformers[torch])\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.2.0 (from torch->transformers[torch])\n","  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/t5_env/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/t5_env/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/t5_env/lib/python3.10/site-packages (from requests->transformers[torch]) (2.2.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/t5_env/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/t5_env/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n","Collecting mpmath>=0.19 (from sympy->torch->transformers[torch])\n","  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n","Using cached accelerate-0.29.2-py3-none-any.whl (297 kB)\n","Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n","Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","Downloading regex-2024.4.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hUsing cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","Using cached torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n","Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n","Using cached filelock-3.13.4-py3-none-any.whl (11 kB)\n","Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n","Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n","Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n","Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n","Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n","Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Installing collected packages: mpmath, sympy, safetensors, regex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, accelerate\n","Successfully installed accelerate-0.29.2 filelock-3.13.4 fsspec-2024.3.1 huggingface-hub-0.22.2 mpmath-1.3.0 networkx-3.3 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 regex-2024.4.16 safetensors-0.4.3 sympy-1.12 tokenizers-0.15.2 torch-2.2.2 transformers-4.39.3 triton-2.2.0\n","Collecting SentencePiece\n","  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n","Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: SentencePiece\n","Successfully installed SentencePiece-0.2.0\n"]}],"source":["!pip install transformers[torch]\n","!pip install SentencePiece\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgZiB6efdeBq","outputId":"d4d78ee0-0844-4ed3-ab46-7bd701e1df01"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/t5_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# Importing required libraries\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Importing the T5 modules from huggingface/transformers\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","from torch.cuda.amp import GradScaler, autocast\n","from transformers import Trainer, TrainingArguments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pFIcG5EmICB"},"outputs":[],"source":["hf_token = \"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\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EdDnZrmA3A0t"},"outputs":[],"source":["scaler = GradScaler()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELq49DTpdeEq"},"outputs":[],"source":["# # Setting up the device for GPU usage\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVbHa7pmdeIV"},"outputs":[],"source":["# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n","class CustomDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.source_len = source_len\n","        self.summ_len = summ_len\n","        self.text = self.data.text\n","        self.ctext = self.data.ctext\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        ctext = str(self.ctext[index])\n","        ctext = ' '.join(ctext.split())\n","\n","        text = str(self.text[index])\n","        text = ' '.join(text.split())\n","\n","        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n","        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n","\n","        source_ids = source['input_ids'].squeeze()\n","        source_mask = source['attention_mask'].squeeze()\n","        target_ids = target['input_ids'].squeeze()\n","        target_mask = target['attention_mask'].squeeze()\n","\n","        return {\n","            'source_ids': source_ids.to(dtype=torch.long),\n","            'source_mask': source_mask.to(dtype=torch.long),\n","            'target_ids': target_ids.to(dtype=torch.long),\n","            'target_ids_y': target_ids.to(dtype=torch.long)\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVU_Nj76eskY"},"outputs":[],"source":["# Creating the training function. This will be called in the main process. It is run depending on the epoch value.\n","# The model is put into train mode and then we enumerate over the training loader and passed to the defined network\n","\n","def train(epoch, tokenizer, model, device, loader, optimizer):\n","    model.train()\n","    for _,data in enumerate(loader, 0):\n","        y = data['target_ids'].to(device, dtype = torch.long)\n","        y_ids = y[:, :-1].contiguous()\n","        lm_labels = y[:, 1:].clone().detach()\n","        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","        ids = data['source_ids'].to(device, dtype = torch.long)\n","        mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n","        loss = outputs[0]\n","\n","        if _%500==0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-CHTTIoh3LZl"},"outputs":[],"source":["# def train(epoch, tokenizer, model, device, loader, optimizer):\n","#     model.train()\n","#     for _, data in enumerate(loader, 0):\n","#         y = data['target_ids'].to(device, dtype=torch.long)\n","#         y_ids = y[:, :-1].contiguous()\n","#         lm_labels = y[:, 1:].clone().detach()\n","#         lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","#         ids = data['source_ids'].to(device, dtype=torch.long)\n","#         mask = data['source_mask'].to(device, dtype=torch.long)\n","\n","#         # Automatic Mixed Precision context\n","#         with autocast():\n","#             outputs = model(input_ids=ids, attention_mask=mask, decoder_input_ids=y_ids, labels=lm_labels)\n","#             loss = outputs.loss\n","\n","#         if _ % 500 == 0:\n","#             print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","\n","#         optimizer.zero_grad()\n","#         # Use scaler to scale the loss for backward pass\n","#         scaler.scale(loss).backward()\n","#         scaler.step(optimizer)\n","#         scaler.update()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcP276H6esm9"},"outputs":[],"source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device, dtype = torch.long)\n","            ids = data['source_ids'].to(device, dtype = torch.long)\n","            mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask,\n","                max_length=150,\n","                num_beams=2,\n","                repetition_penalty=2.5,\n","                length_penalty=1.0,\n","                early_stopping=True\n","                )\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","            if _%100==0:\n","                print(f'Completed {_}')\n","\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","    return predictions, actuals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfKLG9n6espT"},"outputs":[],"source":["# Defining some key variables that will be used later on in the training\n","TRAIN_BATCH_SIZE = 4    # input batch size for training (default: 64)\n","VALID_BATCH_SIZE = 4    # input batch size for testing (default: 1000)\n","TRAIN_EPOCHS = 10        # number of epochs to train (default: 10)\n","VAL_EPOCHS = 1\n","LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","SEED = 42               # random seed (default: 42)\n","MAX_LEN = 256\n","SUMMARY_LEN = 256"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BG3nCt-esrq","outputId":"25a3a3c5-3d39-4078-8d2f-46eae88dca7e"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# Set random seeds and deterministic pytorch for reproducibility\n","torch.manual_seed(SEED) # pytorch random seed\n","np.random.seed(SEED) # numpy random seed\n","torch.backends.cudnn.deterministic = True\n","\n","# tokenzier for encoding the text\n","tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lfeuqzmeesuA","outputId":"eab5aaa3-8db0-4ee8-c3ff-998319dcac1c"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Item Number</th>\n","      <th>Item Text</th>\n","      <th>Summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Item 1</td>\n","      <td>Item 1. Business\\nCompany Background\\nThe Comp...</td>\n","      <td>**Summary:**\\n\\nApple Inc. designs, manufactur...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Item 1A</td>\n","      <td>Item 1A. Risk Factors\\nThe Company’s business,...</td>\n","      <td>The text outlines various risk factors that co...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Item 1B</td>\n","      <td>Item 1B. Unresolved Staff Comments\\nNone.</td>\n","      <td>The text indicates there are no unresolved sta...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Item 2</td>\n","      <td>Item 2. Properties\\nThe Company’s headquarters...</td>\n","      <td>As of September 25, 2021, the company's headqu...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Item 3</td>\n","      <td>Item 3. Legal Proceedings\\nThe Company is subj...</td>\n","      <td>The text describes legal proceedings involving...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ID Item Number                                          Item Text  \\\n","0   0      Item 1  Item 1. Business\\nCompany Background\\nThe Comp...   \n","1   1     Item 1A  Item 1A. Risk Factors\\nThe Company’s business,...   \n","2   2     Item 1B          Item 1B. Unresolved Staff Comments\\nNone.   \n","3   3      Item 2  Item 2. Properties\\nThe Company’s headquarters...   \n","4   4      Item 3  Item 3. Legal Proceedings\\nThe Company is subj...   \n","\n","                                             Summary  \n","0  **Summary:**\\n\\nApple Inc. designs, manufactur...  \n","1  The text outlines various risk factors that co...  \n","2  The text indicates there are no unresolved sta...  \n","3  As of September 25, 2021, the company's headqu...  \n","4  The text describes legal proceedings involving...  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('training_ds.csv')\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IbE5RHF8eswY","outputId":"588cf694-b602-4d19-c26d-3fe286e1c9a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                text  \\\n","0  **Summary:**\\n\\nApple Inc. designs, manufactur...   \n","1  The text outlines various risk factors that co...   \n","2  The text indicates there are no unresolved sta...   \n","3  As of September 25, 2021, the company's headqu...   \n","4  The text describes legal proceedings involving...   \n","\n","                                               ctext  \n","0  summarize: Item 1. Business\\nCompany Backgroun...  \n","1  summarize: Item 1A. Risk Factors\\nThe Company’...  \n","2  summarize: Item 1B. Unresolved Staff Comments\\...  \n","3  summarize: Item 2. Properties\\nThe Company’s h...  \n","4  summarize: Item 3. Legal Proceedings\\nThe Comp...  \n"]}],"source":["df = df.rename(columns={'Item Text': 'ctext', 'Summary':'text'})\n","df = df[['text','ctext']]\n","df.ctext = 'summarize: ' + df.ctext # add prefix \"summarize: \" to input indicating the task\n","print(df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VghwXi2zesyr","outputId":"73925900-1e9a-427b-d0ab-2ad13f0a440d"},"outputs":[{"name":"stdout","output_type":"stream","text":["FULL Dataset: (2248, 2)\n","TRAIN Dataset: (1798, 2)\n","TEST Dataset: (450, 2)\n"]}],"source":["train_size = 0.8\n","train_dataset=df.sample(frac=train_size,random_state = SEED)\n","val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","\n","print(\"FULL Dataset: {}\".format(df.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"TEST Dataset: {}\".format(val_dataset.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-YheFF5ftIN"},"outputs":[],"source":["# Creating the Training and Validation dataset for further creation of Dataloader\n","training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n","val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LiMAkvjpfwIX"},"outputs":[],"source":["# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n","training_loader = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=VALID_BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9mRrdZqfwLB"},"outputs":[],"source":["# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n","# Further this model is sent to device (GPU/TPU) for using the hardware.\n","model = T5ForConditionalGeneration.from_pretrained(\"t5-large\")\n","model = model.to(device)\n","\n","# Defining the optimizer that will be used to tune the weights of the network in the training session.\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VulR8kkifz4k","outputId":"f0757067-b38f-4757-e3d4-9acec471aca2"},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Initiating Fine-Tuning for the model on our dataset\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/t5_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0, Loss:  9.496197700500488\n","Epoch: 1, Loss:  2.243647813796997\n","Epoch: 2, Loss:  1.08051598072052\n","Epoch: 3, Loss:  0.7972398996353149\n","Epoch: 4, Loss:  1.1123881340026855\n","Epoch: 5, Loss:  0.42818599939346313\n","Epoch: 6, Loss:  0.7567224502563477\n","Epoch: 7, Loss:  0.6454167366027832\n","Epoch: 8, Loss:  0.3691837787628174\n","Epoch: 9, Loss:  0.8094171285629272\n"]}],"source":["# Training loop (taking around 22 mins)\n","print('Initiating Fine-Tuning for the model on our dataset')\n","\n","for epoch in range(TRAIN_EPOCHS):\n","    train(epoch, tokenizer, model, device, training_loader, optimizer)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QxrHrWudfz7J","outputId":"0fe59a57-66c6-473e-852f-9e6ee07843ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n","Completed 0\n","Completed 100\n"]}],"source":["print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n","for epoch in range(VAL_EPOCHS):\n","    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zskxTGfgDBR","outputId":"0a35f374-c4cd-4f10-ea78-68bc487b5fe4"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                      Generated Text  \\\n","0  Company's business, reputation, results of ope...   \n","1  Company is involved in legal proceedings and c...   \n","2  information regarding executive compensation f...   \n","3  information regarding security ownership of ce...   \n","4  information regarding Principal Accountant Fee...   \n","\n","                                         Actual Text  \n","0  The text outlines various risk factors that co...  \n","1  The text describes legal proceedings involving...  \n","2  The required information on executive compensa...  \n","3  The text states that the details regarding the...  \n","4  This section of the document provides informat...  \n"]}],"source":["print(final_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e67xNSagfz9e"},"outputs":[],"source":["#to evaluate the generated text using metrics like \"bleu\" and \"rouge\"\n","!pip install evaluate\n","import evaluate\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGplmYe6gGN_"},"outputs":[],"source":["metric = evaluate.load(\"bleu\")\n","references = [ [a] for a in actuals ]\n","results = metric.compute(predictions=predictions, references=references, tokenizer=word_tokenize)\n","results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43Pg_V-zgGQg"},"outputs":[],"source":["!pip install rouge_score\n","metric = evaluate.load('rouge')\n","results = metric.compute(predictions=predictions, references=references, tokenizer=word_tokenize)\n","results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DV2L0bsfgOYP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"naEiScG9kJVp","outputId":"65b733d7-3e52-43a7-a998-89b89c0e3feb"},"outputs":[{"data":{"text/plain":["('t5_large_tokenizer_epochs_10/tokenizer_config.json',\n"," 't5_large_tokenizer_epochs_10/special_tokens_map.json',\n"," 't5_large_tokenizer_epochs_10/spiece.model',\n"," 't5_large_tokenizer_epochs_10/added_tokens.json')"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained('t5_large_epochs_10')\n","tokenizer.save_pretrained('t5_large_tokenizer_epochs_10')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BmFwD-wpmICQ"},"outputs":[],"source":["repo_name = 'T5-large-10K-summarization'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VI2DlAFDmICR"},"outputs":[],"source":["args = TrainingArguments(\n","    output_dir='./results',  # where to save model checkpoints\n","    hub_model_id=f'yatharth97/{repo_name}',  # your HF model repository\n","    push_to_hub=True,  # enables pushing to hub after training, if you train within this script\n","    hub_token=hf_token,  # your Hugging Face API token\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dl40YiSYmICR","outputId":"4f8d37ee-b39c-42a0-fb94-402b796cea63"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/t5_env/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"source":["trainer = Trainer(model=model, tokenizer=tokenizer, args=args)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"euJaSpiYmICR","outputId":"f6af7dc3-0a60-4b9f-ddad-59abf1f5b465"},"outputs":[{"name":"stderr","output_type":"stream","text":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]\n","\u001b[A\n","\n","\u001b[A\u001b[A\n","\n","training_args.bin: 100%|██████████| 4.98k/4.98k [00:00<00:00, 11.5kB/s]\n","spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 1.37MB/s]\n","model.safetensors: 100%|██████████| 2.95G/2.95G [01:42<00:00, 28.9MB/s]\n","\n","Upload 3 LFS files: 100%|██████████| 3/3 [01:42<00:00, 34.23s/it] \n"]},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/yatharth97/T5-large-10K-summarization/commit/6561bb4a726fc8d79c9ed87e8da1b4cfffdfbc97', commit_message='Commit message describing the changes made', commit_description='', oid='6561bb4a726fc8d79c9ed87e8da1b4cfffdfbc97', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["trainer.push_to_hub(\n","    commit_message=\"Commit message describing the changes made\",\n","    blocking=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8VAn8VWmICS"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}